---
abstract: >-
  Accumulated evidence suggests that Large Language Models (LLMs) are beneficial in predicting neural signals related to narrative processing. The way LLMs integrate context over large timescales, however, is fundamentally different from the way the brain does it. In this study, we show that unlike LLMs that apply parallel processing of large contextual windows, the incoming context to the brain is limited to short windows of a few tens of words. We hypothesize that whereas lower-level brain areas process short contextual windows, higher-order areas in the default-mode network (DMN) engage in an online incremental mechanism where the incoming short context is summarized and integrated with information accumulated across long timescales. Consequently, we introduce a novel LLM that instead of processing the entire context at once, it incrementally generates a concise summary of previous information. As predicted, we found that neural activities at the DMN were better predicted by the incremental model, and conversely, lower-level areas were better predicted with short-context-window LLM.
slides: ""
url_pdf: ""
publication_types:
  - "1"
authors:
  - Refael Tikochinski
  - Ariel Goldstein
  - admin
  - Uri Hasson
  - Roi Reichart
author_notes: []
publication: In *bioRxiv*
summary: This study proposes an incremental language model that better captures how the brain processes auditory context over long timescales by summarizing information incrementally, matching neural activities in the neural networks.
url_dataset: "https://www.nature.com/articles/s41597-021-01033-3"
url_project: ""
publication_short: In *bioRxiv*
url_source: "https://www.biorxiv.org/content/10.1101/2024.01.15.575798v2.full"
url_video: ""
title: Incremental Accumulation of Linguistic Context in Artificial and Biological Neural Networks
doi: "https://doi.org/10.1101/2024.01.15.575798"
featured: true
tags: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
date: 2024-05-02T00:00:00.000Z
url_slides: ""
publishDate: 2024-01-17T00:00:00.000Z
url_poster: ""
url_code: ""
---
